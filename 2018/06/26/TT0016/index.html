<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="/js/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>机器学习算法精要 | TinTin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta name="description" content="原文来自：Essentials of Machine Learning Algorithms  广义而言，机器学习算法分三类1.监督学习工作原理：算法从一组给定的预测因子（独立变量）来预测目标/结果变量（或因变量），使用这些变量集生成映射输入到期望输出的函数，不断训练直到模型达到训练数据所需的精度水平。常见算法有：回归（Regression）、决策树（Decision Tree）、随机森林（Ran">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法精要">
<meta property="og:url" content="https://tintingo.github.io/2018/06/26/TT0016/index.html">
<meta property="og:site_name" content="TinTin">
<meta property="og:description" content="原文来自：Essentials of Machine Learning Algorithms  广义而言，机器学习算法分三类1.监督学习工作原理：算法从一组给定的预测因子（独立变量）来预测目标/结果变量（或因变量），使用这些变量集生成映射输入到期望输出的函数，不断训练直到模型达到训练数据所需的精度水平。常见算法有：回归（Regression）、决策树（Decision Tree）、随机森林（Ran">
<meta property="og:image" content="https://tintingo.github.io/images/T0016.jpg">
<meta property="og:updated_time" content="2018-06-29T03:44:01.101Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法精要">
<meta name="twitter:description" content="原文来自：Essentials of Machine Learning Algorithms  广义而言，机器学习算法分三类1.监督学习工作原理：算法从一组给定的预测因子（独立变量）来预测目标/结果变量（或因变量），使用这些变量集生成映射输入到期望输出的函数，不断训练直到模型达到训练数据所需的精度水平。常见算法有：回归（Regression）、决策树（Decision Tree）、随机森林（Ran">
<meta name="twitter:image" content="https://tintingo.github.io/images/T0016.jpg">
  
    <link rel="alternate" href="/atom.xml" title="TinTin" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">

</head>

<script>
var themeMenus = {};

  themeMenus["/"] = "首页"; 

  themeMenus["/archives"] = "归档"; 

  themeMenus["/life"] = "生活馆"; 

  themeMenus["/download"] = "下载"; 

  themeMenus["https://tintingo.github.io/images/about.html"] = "关于"; 

  themeMenus["/comments"] = "留言"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="TinTin" rel="home"> TinTin </a>
            
          </h1>

          
            <div class="site-description">学习matlab/python/HTML5/c/c++/机器学习/贝叶斯网络中~~~</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/life">生活馆</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/download">下载</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="https://tintingo.github.io/images/about.html">关于</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/comments">留言</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose1.jpg,css/images/pose2.jpg,css/images/pose3.jpg".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-TT0016" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" href="/images/T0016.jpg" rel="gallery_cjj5cw005000eokeumrbopado">
        <img src="/images/T0016.jpg" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      机器学习算法精要
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2018/06/26/TT0016/" class="article-date">
	  <time datetime="2018-06-26T02:43:28.189Z" itemprop="datePublished">June 26, 2018</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/top.jpg" alt=""><br>原文来自：<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" target="_blank" rel="external">Essentials of Machine Learning Algorithms </a></p>
<h1 id="广义而言，机器学习算法分三类"><a href="#广义而言，机器学习算法分三类" class="headerlink" title="广义而言，机器学习算法分三类"></a>广义而言，机器学习算法分三类</h1><h2 id="1-监督学习"><a href="#1-监督学习" class="headerlink" title="1.监督学习"></a>1.监督学习</h2><p>工作原理：算法从一组给定的预测因子（独立变量）来预测目标/结果变量（或因变量），使用这些变量集生成映射输入到期望输出的函数，不断训练直到模型达到训练数据所需的精度水平。常见算法有：回归（Regression）、决策树（Decision Tree）、随机森林（Random Forest）、KNN、逻辑回归（Logistic Regression）等。</p>
<h2 id="2-无监督学习"><a href="#2-无监督学习" class="headerlink" title="2.无监督学习"></a>2.无监督学习</h2><p>工作原理：算法中没有任何目标或结果变量要预测/估计，而是用于不同群体的种群聚集，广泛应用于不同群体的细分。常见算法有：关联规则算法（Apriori algorithm）、K-means。</p>
<h2 id="3-增强学习"><a href="#3-增强学习" class="headerlink" title="3.增强学习"></a>3.增强学习</h2><p>工作原理：算法中，机器被训练来做出特定的决定，通过反复试验不断训练机器，机器从过去的经验中学习，捕捉最好的知识以做出准确的决策。常见算法有：马尔科夫决策过程(Markov Decision Process)。<br><a id="more"></a></p>
<h1 id="常用机器学习算法"><a href="#常用机器学习算法" class="headerlink" title="常用机器学习算法"></a>常用机器学习算法</h1><p>1.线性回归 - Linear Regression<br>2.逻辑回归 - Logistic Regression<br>3.决策树 - Decision Tree<br>4.支持向量机 - SVM<br>5.朴素贝叶斯 - Naive Bayes<br>6.邻近算法 - kNN<br>7.K均值 - K-Means<br>8.随机森林 - Random Forest<br>9.降维算法 - Dimensionality Reduction Algorithms<br>10.梯度提升算法 - Gradient Boosting algorithms （GBM、XGBoost、LightGBM、CatBoost）</p>
<h1 id="一、线性回归（Linear-Regression）"><a href="#一、线性回归（Linear-Regression）" class="headerlink" title="一、线性回归（Linear Regression）"></a>一、线性回归（Linear Regression）</h1><p>线性回归是用连续变量来估计实际值（房屋成本、通话次数、总销售额等）,通过拟合一条最佳的线来建立自变量和因变量之间的关系，这个最佳拟合线称为回归线，用线性方程y = a*x + b表示。<br>了解线性回归的最好方法是重温童年的经历。要求一个第五年级的孩子通过体重的递增顺序来给他班上的学生排队，在不给出他们的体重的情况下，你认为这个孩子会怎么做？他（她）可能会观察他们的身高和身材（视觉分析），并综合这些可见参数给他们排队。这就是现实生活中的线性回归！这个孩子实际上已经知道身高和身材与体重的关系是由一种像上面的等式关系决定的。<br>等式中，y为因变量，x为自变量，a为斜率，b为截距。系数a和b可通过极小化数据点与回归线之间的距离的平方差之和获得。<br>如下例中，已经确定了线性方程y = 0.2811x + 13.9为最佳拟合线，用这个方程式，通过体重，就可知道一个人的身高。<br><img src="/images/JQXX001.png" alt=""><br>线性回归主要有两类：简单线性回归和多元线性回归。简单线性回归的特点是一个自变量，多元线性回归的特征是多个（1个以上）自变量。在寻找最佳拟合线时，可以拟合多项式或曲线回归，这被称为多项式或曲线回归。<br>python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line">from sklearn import linear_model</div><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train=input_variables_values_training_datasets</div><div class="line">y_train=target_variables_values_training_datasets</div><div class="line">x_test=input_variables_values_test_datasets</div><div class="line"><span class="comment"># Create linear regression object</span></div><div class="line">linear = linear_model.LinearRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear.fit(x_train, y_train)</div><div class="line">linear.score(x_train, y_train)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: \n'</span>, linear.coef_)</div><div class="line"><span class="built_in">print</span>(<span class="string">'Intercept: \n'</span>, linear.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= linear.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train &lt;- input_variables_values_training_datasets</div><div class="line">y_train &lt;- target_variables_values_training_datasets</div><div class="line">x_test &lt;- input_variables_values_test_datasets</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear &lt;- lm(y_train ~ ., data = x)</div><div class="line">summary(linear)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(linear,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="二、逻辑回归（Logistic-Regression）"><a href="#二、逻辑回归（Logistic-Regression）" class="headerlink" title="二、逻辑回归（Logistic Regression）"></a>二、逻辑回归（Logistic Regression）</h1><p>不要被它的名字混淆！它是一种分类而不是回归算法。它是用来估计基于给定自变量集的离散值（二进制值，如0/1，是/否，真/假）。简单地说，它通过将数据拟合到对数函数来预测事件发生的概率，因此，它也被称为对数回归。由于它预测了概率，其输出值介于0和1之间。<br>再一次通过一个简单的例子来理解一下。<br>假设你的朋友给你一个难题来解决，只有2种结果：解决、没解决。现在想象一下，机器通过给你做各种各样的测试来试图了解你擅长的科目，学习的结果将是这样的：如果给你一个十年级的三重测量问题，你有70%的概率来解决这个问题。另一方面，如果是五年级的历史问题，你只有30%的概率来解决这个问题。<br>在数学中，结果的对数几率（log odds：对数几率odds是体现阳性和阴性差异的这么一个指标）被模拟成预测变量的线性组合。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">odds = p/ (1-p)          【事件发生可能性 / 事件发生可能性】</div><div class="line">ln(odds) = ln(p/(1-p))</div><div class="line">logit(p) = ln(p/(1-p)) = b0 + b1X1 + b2X2 + b3X3 + ... + bkXk</div></pre></td></tr></table></figure></p>
<p>如上，P是存在目标特征的概率，它选择观察样本值的最大似然参数，而不选择普通回归中最小平方误差和。<br>现在，你可能会问，为什么要选择log函数？为了简单起见，只能说这是复制阶跃函数最好的数学方法之一。<br><img src="/images/JQXX002.png" alt=""><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.linear_model import LogisticRegression</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: \n'</span>, model.coef_)</div><div class="line"><span class="built_in">print</span>(<span class="string">'Intercept: \n'</span>, model.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</div><div class="line">summary(logistic)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(logistic,x_test)</div></pre></td></tr></table></figure></p>
<p>除此之外，为了改进模型，还可以尝试更多不同的步骤：包括交互项、去除特征、正则化、使用非线性模型。</p>
<h1 id="三、决策树（Decision-Tree）"><a href="#三、决策树（Decision-Tree）" class="headerlink" title="三、决策树（Decision Tree）"></a>三、决策树（Decision Tree）</h1><p>决策树是一种主要用于分类问题的监督学习算法，它适用于分类和连续因变量。在该算法中，我们将种群分成两个或更多的同质集，采用关键属性/自变量来尽可能区分的不同组。<br><img src="/images/JQXX003.png" alt=""><br>在上图中，你可以看到，基于多个属性，种群被划分为四个不同的组，以确定“他们是否会玩”，采用Gini、信息增益、Chi平方、熵等多种算法将种群划分为不同的异质集。<br>了解决策树是如何运作的最好方法是玩Jezzball——一个经典的微软游戏（下图）。本质上，你有一个带有可移动墙的房间，你需要创建墙壁能够清除最大的没有球的区域。<br><img src="/images/JQXX004.png" alt=""><br>所以，每次你用墙分开房间时，你试图在同一个房间里创造2个不同的群体。决策树方式工作非常相似，就是尽可能的将种群划分为不同的组。<br>了解更多：<a href="https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/" target="_blank" rel="external">简化的决策树算法版本</a><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line">from sklearn import tree</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create tree object </span></div><div class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>) <span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></div><div class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(rpart)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># grow tree </span></div><div class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="四、SVM-Support-Vector-Machine"><a href="#四、SVM-Support-Vector-Machine" class="headerlink" title="四、SVM (Support Vector Machine)"></a>四、SVM (Support Vector Machine)</h1><p>SVM是一种分类方法。在该算法中，我们将每个数据项绘制为n维空间中的一个点（其中n是具有的特征数），其中每个特征的值是特定坐标的值。<br>例如，若我们只有两个特征，身高和头发长度，我们首先在二维空间中绘制这两个变量，其中每个点有两个坐标（这些坐标被称为支持向量）。<br><img src="/images/JQXX005.png" alt=""><br>现在，我们将找到一条分割两个不同分类数据集之间的数据的线，这是一条直线，使得两组中最靠近的点的距离最远。<br><img src="/images/JQXX006.png" alt=""><br>在上面的例子中，将数据分割成两个不同的类的线是黑线，因为这两个最靠近的点离直线最远，这条线就是我们的分类器。这样，看测试数据落在该线的哪一边，就可将新数据分为什么类。<br>了解更多：<a href="https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/" target="_blank" rel="external">支持向量机的简化版本</a><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn import svm</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object </span></div><div class="line">model = svm.svc() <span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-svm(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="五、朴素贝叶斯"><a href="#五、朴素贝叶斯" class="headerlink" title="五、朴素贝叶斯"></a>五、朴素贝叶斯</h1><p>朴素贝叶斯是一种基于贝叶斯定理的分类算法，具有预测属性之间的独立性假设。简单地说，朴素贝叶斯分类器假定类中的特定特征的存在与任何其他特征的存在无关。例如，如果一个水果是红色的、圆的、直径约3英寸的，它可以被认为是一个苹果。即使这些特征彼此依赖或存在其他特征，朴素贝叶斯分类器也认为所有这些属性对“这种水果是苹果”贡献的概率是独立的。<br>朴素贝叶斯模型易于建立，特别适用于非常大的数据集。由于它简单，朴素贝叶斯甚至被认为优于其他高度复杂的分类方法。<br>贝叶斯定理为通过P(C)、P(X)和P(X|C)来计算后验概率p(C|x)提供了一种途径。请看下面的方程式：<br><img src="/images/JQXX007.png" alt=""><br>其中，P(C|x)是给定（待测）属性的类（目标）的后验概率,P(C)是类的先验概率,P(x|c)是给定类情况下待测属性出现的概率,P(x)是待测属性的先验概率。<br>例：有一个包含天气和相应的目标变量“玩”的训练数据集，现在，我们需要根据天气情况来判断是否可以去玩。<br>按如下步骤执行：<br>第1步：将数据集转换为频率表<br>第2步：通过比如阴天的概率为0.29和去玩的概率为0.64来创建似然表。<br><img src="/images/JQXX008.png" alt=""><br>第3步：使用朴素贝叶斯公式计算每个类的后验概率，具有最高后验概率的类就是预测结果。<br>问题：如果天气晴朗会去玩，这个说法正确吗？<br>我们可以用上面讨论的方法求解它，P(Yes | Sunny) = P( Sunny | Yes) <em> P(Yes) / P (Sunny)<br>表中可知：P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P(Yes)= 9/14 = 0.64<br>那么，P (Yes | Sunny) = 0.33 </em> 0.64 / 0.36 = 0.60，具有较高的概率。<br>朴素贝叶斯使用类似的方法来预测各属性的不同类别的概率，该算法主要用于文本分类和多分类问题。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.naive_bayes import GaussianNB</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="六、kNN-k-Nearest-Neighbors"><a href="#六、kNN-k-Nearest-Neighbors" class="headerlink" title="六、kNN (k- Nearest Neighbors)"></a>六、kNN (k- Nearest Neighbors)</h1><p>KNN可以用于分类和回归问题，而在工业上常用在分类问题上。K近邻算法是一种简单的算法，它存储了所有可用的样本，并通过其k领域的多数表决对新的样本进行分类。样本的分类是K近邻范围内所有点通过距离函数测量的多数共识。<br>这些距离函数可以是欧几里得（Euclidean）、曼哈顿（Manhattan）、闵可夫斯基（Minkowski）和汉明（Hamming）距离，前三个函数用于连续函数，第四个（Hamming）用于分类变量，如果k＝1，则将该情况简单地分给其最近邻的类。在进行KNN建模时，选择K值是最大的难点。<br><img src="/images/JQXX009.png" alt=""><br>KNN可以很容易地映射到我们的真实生活中，如果你想了解一个人，你没有他/她的信息，你可能从他的亲密朋友和他的圈子获得他/她的信息。<br>选择KNN之前要考虑的事项：1.KNN运行较耗时；2.变量需归一化，否则较高的范围变量会对它产生偏差；3.在KNN之前需进行如离群点、去噪的预处理工作。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.neighbors import KNeighborsClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">KNeighborsClassifier(n_neighbors=6) <span class="comment"># default value for n_neighbors is 5</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(knn)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-knn(y_train ~ ., data = x,k=5)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="七、K-Means"><a href="#七、K-Means" class="headerlink" title="七、K-Means"></a>七、K-Means</h1><p>K-Means是一种解决聚类问题的无监督算法。它的过程遵循一种简单且容易的方式，通过一定数量的簇（给定k簇）对给定的数据集进行分类。集群内的数据点对于每组是同质的和异质的。<br>记得从墨迹中找出形状吗？K-Means就能做类似的活动，通过观察形状能够辨认出多少不同的簇/种群存在。<br><img src="/images/JQXX010.png" alt=""><br>K-means如何形成集群：<br>1.K-means为每个集群挑选K个点，称为质心。<br>2.每个数据点形成具有最接近质心的簇，即K簇。<br>3.根据现有的群集成员查找每个簇的质心，以获得新的质心。<br>4.当有了新的质心时，重复第2步和第3步。从新的质心找到每个数据点最近的距离，并与新的K簇关联。重复这个过程直到收敛，即质心不变。<br>如何确定K值：<br>在K-means中有簇，且每个簇都有它自己的质心。簇中质心与数据点之间差的平方之和构成该簇的平方值之和，当所有簇的平方值之和被添加时，也就有了簇解的平方值总和。<br>我们知道，随着簇的数量增加，这个值持续减小，但若绘出结果图，会看到平方距离的总和急剧下降到某个值：K，然后缓慢下降，这样就可以找到最佳的簇数。<br><img src="/images/JQXX011.png" alt=""><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.cluster import KMeans</div><div class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">k_means = KMeans(n_clusters=3, random_state=0)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">library(cluster)</div><div class="line">fit &lt;- kmeans(X, 3) <span class="comment"># 5 cluster solution</span></div></pre></td></tr></table></figure></p>
<h1 id="八、随机森林"><a href="#八、随机森林" class="headerlink" title="八、随机森林"></a>八、随机森林</h1><p>随机森林是一个包含多个决策树的分类器，它是决策树的集合（被称为“森林”）。为了根据属性对新对象进行分类，每个树都给出分类，并称树为该类“投票”，森林选择选票最多的分类（遍及森林中的所有树）。<br>树生成如下：<br>1.如果训练集中的样本数为N，则随机抽取N个样本，但有放回，这个样本便是生成树的训练集。<br>2.如果有M个输入变量，在每个节点处指定一个远小于M的数m，从M中随机选择m个变量，并这些m上的最佳分割用于分割节点。在森林生长过程中，m的值保持不变。<br>3.每棵树尽可能最大的生长，没有修剪。<br>关于该算法的更多细节，与决策树和调谐模型参数进行比较，我建议您阅读这些文章：<br>1.<a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="external">随机森林简介</a><br>2.<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/" target="_blank" rel="external">CART模型与随机森林的比较（第1部分）</a><br>3.<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/" target="_blank" rel="external">随机森林与CART模型的比较（第2部分）</a><br>4.<a href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/" target="_blank" rel="external">调整随机森林模型的参数</a><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.ensemble import RandomForestClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Random Forest object</span></div><div class="line">model= RandomForestClassifier()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(randomForest)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;- randomForest(Species ~ ., x,ntree=500)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h1 id="九、降维算法"><a href="#九、降维算法" class="headerlink" title="九、降维算法"></a>九、降维算法</h1><p>在过去的4-5年中，数据捕获在每一个可能的阶段都呈指数增长。企业/政府机构/研究机构不仅不断地有新的资源出现，而且还要非常详细地捕捉数据。<br>比如电子商务公司正在捕捉更多关于客户的细节，如他们的人口数据、网络浏览历史、他们喜欢或不喜欢的东西、购买历史、反馈和其他许多信息，以比他们最近的杂货店店主更个性化地关注他们。<br>作为一个数据科学家，我们所提供的数据也包含许多特征，这对于建立良好的鲁棒模型很有帮助，但是存在一个挑战，如何从一两千的数据中找到较主要的变量？在这种情况下，降维算法有助于结合决策树、随机森林、PCA、因子分析、基于相关矩阵、缺失值比率等多种算法进行识别。<br>要知道更多关于这个算法，你可以阅读“<a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/" target="_blank" rel="external">学习降维算法初学者指南</a>”。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn import decomposition</div><div class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></div><div class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></div><div class="line"><span class="comment"># For Factor analysis</span></div><div class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></div><div class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></div><div class="line">train_reduced = pca.fit_transform(train)</div><div class="line"><span class="comment">#Reduced the dimension of test dataset</span></div><div class="line">test_reduced = pca.transform(<span class="built_in">test</span>)</div><div class="line"><span class="comment">#For more detail on this, please refer  this link.</span></div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">library(stats)</div><div class="line">pca &lt;- princomp(train, cor = TRUE)</div><div class="line">train_reduced  &lt;- predict(pca,train)</div><div class="line">test_reduced  &lt;- predict(pca,<span class="built_in">test</span>)</div></pre></td></tr></table></figure></p>
<h1 id="十、梯度提升算法"><a href="#十、梯度提升算法" class="headerlink" title="十、梯度提升算法"></a>十、梯度提升算法</h1><h2 id="十点一、GBM"><a href="#十点一、GBM" class="headerlink" title="十点一、GBM"></a>十点一、GBM</h2><p>GBM是一种利用大量数据进行预测的Boosting算法（Boosting算法是一种用来提高弱分类算法准确度的方法），具有很高的预测能力。实际上，Boosting是学习算法的一种综合，它结合了多个基估计的预测，以提高单个估计器的鲁棒性。它将多个弱预测因子或平均预测因子组合成一个强预测因子。这些Boosting算法在Kaggle、AV Hakason、CrowdAnalytix等数据科学竞赛中有很好。<br>了解更多：<a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="external">详细了解Boosting算法</a><br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line">from sklearn.ensemble import GradientBoostingClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></div><div class="line">model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">library(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = 4, repeats = 4)</div><div class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = FALSE)</div><div class="line">predicted= predict(fit,x_test,<span class="built_in">type</span>= <span class="string">"prob"</span>)[,2]</div></pre></td></tr></table></figure></p>
<p>梯度提升分类器和随机森林是两种不同的提升树分类器，<a href="https://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341" target="_blank" rel="external">两种算法间的差异</a>。</p>
<h2 id="十点二、XGBoost"><a href="#十点二、XGBoost" class="headerlink" title="十点二、XGBoost"></a>十点二、XGBoost</h2><p>另一种经典的梯度提升算法通常是一些Kaggle比赛中成败的决定性选择。<br>XGBoost具有非常高的预测能力，这使得它成为事件精度的最佳选择。因为它同时具有线性模型和树学习算法，使得该算法比现有的梯度提升算法快近10倍。<br>XGBoost支持包括各种目标函数，包括回归、分类和排序。<br>XGBoost最有趣的事情之一是它也被称为一种正则提升算法，它助于减少过拟合建模，并且对Scala、Java、R、Python、Julia和C++等多种语言有很好的支持。<br>支持分布式训练和广泛训练，包括GCE，AWS，Azure和Yarn-clusters等许多机器。XGBoosting还可以与Spark、Flink和其他云数据流系统集成，并在Boosting进程的每一次迭代中进行内置的交叉验证。<br>有关XGBoost和参数调整的更多信息，<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="external">点这里</a>。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">from xgboost import XGBClassifier</div><div class="line">from sklearn.model_selection import train_test_split</div><div class="line">from sklearn.metrics import accuracy_score</div><div class="line">X = dataset[:,0:10]</div><div class="line">Y = dataset[:,10:]</div><div class="line">seed = 1</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)</div><div class="line">model = XGBClassifier()</div><div class="line">model.fit(X_train, y_train)</div><div class="line"><span class="comment">#Make predictions for test data</span></div><div class="line">y_pred = model.predict(X_test)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">require(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">TrainControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = 10, repeats = 4)</div><div class="line">model&lt;- train(y ~ ., data = x, method = <span class="string">"xgbLinear"</span>, trControl = TrainControl,verbose = FALSE)</div><div class="line">OR </div><div class="line">model&lt;- train(y ~ ., data = x, method = <span class="string">"xgbTree"</span>, trControl = TrainControl,verbose = FALSE)</div><div class="line">predicted &lt;- predict(model, x_test)</div></pre></td></tr></table></figure></p>
<h2 id="十点三、LightGBM"><a href="#十点三、LightGBM" class="headerlink" title="十点三、LightGBM"></a>十点三、LightGBM</h2><p>LightGBM是一种使用基于树的学习算法的梯度提升框架。它被设计成分布式的、高效的，优点如下：<br>.更快的训练速度和更高的效率<br>.低内存<br>.高精度<br>.支持并行GPU学习<br>.能够处理大规模数据<br>该框架是一种基于决策树算法的快速和高性能的梯度提升算法，用于排序、分类和许多其他机器学习任务。它是在微软的分布式机器学习工具包项目下开发的。<br>由于LightGBM是一种基于决策树的算法，LightGBM使用leaf-wise的树生长策略，而很多其他boosting算法采用depth-wise和level-wise的树生长策略。因此，LightGBM中，当在同一片叶子上生长时，leaf-wise算法可以比level-wise算法减少更多的损失，从而得到更高的精度，这是任何现有的Boosting算法很少能实现的。与depth-wise和level-wise的树生长策略相比，leaf-wise策略可以收敛的更快。<br>此外，因为LightGBM算法速度非常快，因此用“Light”这个词。<br>参考文章了解更多关于LightGBM：<a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" target="_blank" rel="external">点这里</a>。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">data = np.random.rand(500, 10) <span class="comment"># 500 entities, each contains 10 features</span></div><div class="line">label = np.random.randint(2, size=500) <span class="comment"># binary target</span></div><div class="line">train_data = lgb.Dataset(data, label=label)</div><div class="line">test_data = train_data.create_valid(<span class="string">'test.svm'</span>)</div><div class="line">param = &#123;<span class="string">'num_leaves'</span>:31, <span class="string">'num_trees'</span>:100, <span class="string">'objective'</span>:<span class="string">'binary'</span>&#125;</div><div class="line">param[<span class="string">'metric'</span>] = <span class="string">'auc'</span></div><div class="line">num_round = 10</div><div class="line">bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])</div><div class="line">bst.save_model(<span class="string">'model.txt'</span>)</div><div class="line"><span class="comment"># 7 entities, each contains 10 features</span></div><div class="line">data = np.random.rand(7, 10)</div><div class="line">ypred = bst.predict(data)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">library(RLightGBM)</div><div class="line">data(example.binary)</div><div class="line"><span class="comment">#Parameters</span></div><div class="line">num_iterations &lt;- 100</div><div class="line">config &lt;- list(objective = <span class="string">"binary"</span>,  metric=<span class="string">"binary_logloss,auc"</span>, learning_rate = 0.1, num_leaves = 63, tree_learner = <span class="string">"serial"</span>, feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)</div><div class="line"><span class="comment">#Create data handle and booster</span></div><div class="line">handle.data &lt;- lgbm.data.create(x)</div><div class="line">lgbm.data.setField(handle.data, <span class="string">"label"</span>, y)</div><div class="line">handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))</div><div class="line"><span class="comment">#Train for num_iterations iterations and eval every 5 steps</span></div><div class="line">lgbm.booster.train(handle.booster, num_iterations, 5)</div><div class="line"><span class="comment">#Predict</span></div><div class="line">pred &lt;- lgbm.booster.predict(handle.booster, x.test)</div><div class="line"><span class="comment">#Test accuracy</span></div><div class="line">sum(y.test == (y.pred &gt; 0.5)) / length(y.test)</div><div class="line"><span class="comment">#Save model (can be loaded again via lgbm.booster.load(filename))</span></div><div class="line">lgbm.booster.save(handle.booster, filename = <span class="string">"/tmp/model.txt"</span>)</div></pre></td></tr></table></figure></p>
<p>如果您熟悉R中的Caret包的话，以下是实现LightGBM的另一种方式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">require(caret)</div><div class="line">require(RLightGBM)</div><div class="line">data(iris)</div><div class="line">model &lt;-caretModel.LGBM()</div><div class="line">fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = 0)</div><div class="line"><span class="built_in">print</span>(fit)</div><div class="line">y.pred &lt;- predict(fit, iris[,1:4])</div><div class="line">library(Matrix)</div><div class="line">model.sparse &lt;- caretModel.LGBM.sparse()</div><div class="line"><span class="comment">#Generate a sparse matrix</span></div><div class="line">mat &lt;- Matrix(as.matrix(iris[,1:4]), sparse = T)</div><div class="line">fit &lt;- train(data.frame(idx = 1:nrow(iris)), iris<span class="variable">$Species</span>, method = model.sparse, matrix = mat, verbosity = 0)</div><div class="line"><span class="built_in">print</span>(fit)</div></pre></td></tr></table></figure></p>
<h2 id="十点四、Catboost"><a href="#十点四、Catboost" class="headerlink" title="十点四、Catboost"></a>十点四、Catboost</h2><p>Catboost是最近从Yandex开源的机器学习算法，它可以很容易地与谷歌的TensorFlow和苹果的Core ML等深度学习框架相结合。<br>关于Catboost最好的部分是，它不像其他ML模型那样需要大量的数据训练，并且可以工作在各种数据格式上，而不破坏它的鲁棒性。<br>在执行之前，务必确保处理好缺失数据。<br>CATBooST可以自动处理分类变量而不显示类型的转换误差，这有助于您更好地调整模型，而不是去挑出微小的误差。<br>了解更多关于Catboost：<a href="https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/" target="_blank" rel="external">从这篇文章</a>。<br>Python代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line">from catboost import CatBoostRegressor</div><div class="line"><span class="comment">#Read training and testing files</span></div><div class="line">train = pd.read_csv(<span class="string">"train.csv"</span>)</div><div class="line"><span class="built_in">test</span> = pd.read_csv(<span class="string">"test.csv"</span>)</div><div class="line"><span class="comment">#Imputing missing values for both train and test</span></div><div class="line">train.fillna(-999, inplace=True)</div><div class="line">test.fillna(-999,inplace=True)</div><div class="line"><span class="comment">#Creating a training set for modeling and validation set to check model performance</span></div><div class="line">X = train.drop([<span class="string">'Item_Outlet_Sales'</span>], axis=1)</div><div class="line">y = train.Item_Outlet_Sales</div><div class="line">from sklearn.model_selection import train_test_split</div><div class="line">X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)</div><div class="line">categorical_features_indices = np.where(X.dtypes != np.float)[0]</div><div class="line"><span class="comment">#importing library and building model</span></div><div class="line">from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function=<span class="string">'RMSE'</span>)</div><div class="line">model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)</div><div class="line">submission = pd.DataFrame()</div><div class="line">submission[<span class="string">'Item_Identifier'</span>] = <span class="built_in">test</span>[<span class="string">'Item_Identifier'</span>]</div><div class="line">submission[<span class="string">'Outlet_Identifier'</span>] = <span class="built_in">test</span>[<span class="string">'Outlet_Identifier'</span>]</div><div class="line">submission[<span class="string">'Item_Outlet_Sales'</span>] = model.predict(<span class="built_in">test</span>)</div></pre></td></tr></table></figure></p>
<p>R代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">set.seed(1)</div><div class="line">require(titanic)</div><div class="line">require(caret)</div><div class="line">require(catboost)</div><div class="line">tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]</div><div class="line">data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)</div><div class="line">drop_columns = c(<span class="string">"PassengerId"</span>, <span class="string">"Survived"</span>, <span class="string">"Name"</span>, <span class="string">"Ticket"</span>, <span class="string">"Cabin"</span>)</div><div class="line">x &lt;- data[,!(names(data) %<span class="keyword">in</span>% drop_columns)]y &lt;- data[,c(<span class="string">"Survived"</span>)]</div><div class="line">fit_control &lt;- trainControl(method = <span class="string">"cv"</span>, number = 4,classProbs = TRUE)</div><div class="line">grid &lt;- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3,            rsm = 0.95, border_count = 64)</div><div class="line">report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)</div><div class="line"><span class="built_in">print</span>(report)</div><div class="line">importance &lt;- varImp(report, scale = FALSE)</div><div class="line"><span class="built_in">print</span>(importance)</div></pre></td></tr></table></figure></p>
<p>至止，你将对常用的机器学习算法有所了解。<br><img src="/images/bottom.jpg" alt=""></p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
      
      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: '打赏支持', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: '/css/images/wechatpay.jpg',
  alipayImage: '/css/images/alipay.jpg'
});
</script>
      
            
      
	  
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8zMjU1NS85MTE2">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/06/11/TT0015/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">大学发明专利申请</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#广义而言，机器学习算法分三类"><span class="nav-number">1.</span> <span class="nav-text">广义而言，机器学习算法分三类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-监督学习"><span class="nav-number">1.1.</span> <span class="nav-text">1.监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-无监督学习"><span class="nav-number">1.2.</span> <span class="nav-text">2.无监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-增强学习"><span class="nav-number">1.3.</span> <span class="nav-text">3.增强学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常用机器学习算法"><span class="nav-number">2.</span> <span class="nav-text">常用机器学习算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一、线性回归（Linear-Regression）"><span class="nav-number">3.</span> <span class="nav-text">一、线性回归（Linear Regression）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、逻辑回归（Logistic-Regression）"><span class="nav-number">4.</span> <span class="nav-text">二、逻辑回归（Logistic Regression）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、决策树（Decision-Tree）"><span class="nav-number">5.</span> <span class="nav-text">三、决策树（Decision Tree）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#四、SVM-Support-Vector-Machine"><span class="nav-number">6.</span> <span class="nav-text">四、SVM (Support Vector Machine)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#五、朴素贝叶斯"><span class="nav-number">7.</span> <span class="nav-text">五、朴素贝叶斯</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#六、kNN-k-Nearest-Neighbors"><span class="nav-number">8.</span> <span class="nav-text">六、kNN (k- Nearest Neighbors)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#七、K-Means"><span class="nav-number">9.</span> <span class="nav-text">七、K-Means</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#八、随机森林"><span class="nav-number">10.</span> <span class="nav-text">八、随机森林</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#九、降维算法"><span class="nav-number">11.</span> <span class="nav-text">九、降维算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#十、梯度提升算法"><span class="nav-number">12.</span> <span class="nav-text">十、梯度提升算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#十点一、GBM"><span class="nav-number">12.1.</span> <span class="nav-text">十点一、GBM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十点二、XGBoost"><span class="nav-number">12.2.</span> <span class="nav-text">十点二、XGBoost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十点三、LightGBM"><span class="nav-number">12.3.</span> <span class="nav-text">十点三、LightGBM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十点四、Catboost"><span class="nav-number">12.4.</span> <span class="nav-text">十点四、Catboost</span></a></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2018 TinTin All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        主题： <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/life" class="mobile-nav-link">life</a>
  
    <a href="/download" class="mobile-nav-link">download</a>
  
    <a href="https://tintingo.github.io/images/about.html" class="mobile-nav-link">About</a>
  
    <a href="/comments" class="mobile-nav-link">comment</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1271468861&web_id=1271468861" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>







  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
